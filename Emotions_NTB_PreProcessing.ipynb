{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basics Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Sentiment\n",
       "0      i can go from feeling so hopeless to so damned...   sadness\n",
       "1       im grabbing a minute to post i feel greedy wrong     anger\n",
       "2      i am ever feeling nostalgic about the fireplac...      love\n",
       "3                                   i am feeling grouchy     anger\n",
       "4      ive been feeling a little burdened lately wasn...   sadness\n",
       "...                                                  ...       ...\n",
       "15994  i just had a very brief time in the beanbag an...   sadness\n",
       "15995  i am now turning and i feel pathetic that i am...   sadness\n",
       "15996                     i feel strong and good overall       joy\n",
       "15997  i feel like this was such a rude comment and i...     anger\n",
       "15998  i know a lot but i feel so stupid because i ca...   sadness\n",
       "\n",
       "[15999 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in training data from CSV\n",
    "df_train = pd.read_csv('./Emotions/train.txt', sep=';')\n",
    "df_train.columns =['Text', 'Sentiment']\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8833</th>\n",
       "      <td>i die wont some man make me feel that lifes wo...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>i still second guess myself and still have a t...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15839</th>\n",
       "      <td>i feel sorry for her father</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6321</th>\n",
       "      <td>i guess which meant or so i assume no photos n...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14912</th>\n",
       "      <td>i was feeling when nick broke up with me over</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12639</th>\n",
       "      <td>i cherish the heartbreak more then the love th...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15530</th>\n",
       "      <td>i feel that the people i have allocated my que...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>i really wanna see her soon but i feel really ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>is that you feel it more than hear it and the ...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>i find it hard to breathe and sometimes feel a...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Sentiment\n",
       "8833   i die wont some man make me feel that lifes wo...       joy\n",
       "223    i still second guess myself and still have a t...       joy\n",
       "15839                        i feel sorry for her father   sadness\n",
       "6321   i guess which meant or so i assume no photos n...   sadness\n",
       "14912      i was feeling when nick broke up with me over   sadness\n",
       "...                                                  ...       ...\n",
       "12639  i cherish the heartbreak more then the love th...   sadness\n",
       "15530  i feel that the people i have allocated my que...       joy\n",
       "10109  i really wanna see her soon but i feel really ...   sadness\n",
       "760    is that you feel it more than hear it and the ...      love\n",
       "5655   i find it hard to breathe and sometimes feel a...      fear\n",
       "\n",
       "[2400 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the training set to ~3,000 samples\n",
    "percentage = 0.15\n",
    "df_train_downsampled = df_train.sample(frac=percentage,random_state=0)\n",
    "df_train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i cant walk into a shop anywhere where i do no...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>i just keep feeling like someone is being unki...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>im feeling a little cranky negative after this...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>i feel that i am useful to my people and that ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>im feeling more comfortable with derby i feel ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>i feel all weird when i have to meet w people ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Sentiment\n",
       "0             im updating my blog because i feel shitty   sadness\n",
       "1     i never make her separate from me because i do...   sadness\n",
       "2     i left with my bouquet of red and yellow tulip...       joy\n",
       "3       i was feeling a little vain when i did this one   sadness\n",
       "4     i cant walk into a shop anywhere where i do no...      fear\n",
       "...                                                 ...       ...\n",
       "1994  i just keep feeling like someone is being unki...     anger\n",
       "1995  im feeling a little cranky negative after this...     anger\n",
       "1996  i feel that i am useful to my people and that ...       joy\n",
       "1997  im feeling more comfortable with derby i feel ...       joy\n",
       "1998  i feel all weird when i have to meet w people ...      fear\n",
       "\n",
       "[1999 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in testing data from CSV\n",
    "df_test = pd.read_csv('./Emotions/test.txt', sep=';')\n",
    "df_test.columns =['Text', 'Sentiment']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>i feel so honored today and i want to share th...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>i noticed that i was feeling very stressed and...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>i feel safe with berry</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>im feeling quite positive in what i want to ac...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>i want to say i feel numb but if i was numb i ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>im feeling pretty disheartened by the whole thing</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>i start to feel lethargic about blogging</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>i had told gerry yesterday that if i feel isol...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>i type these words i feel like i shouldn t be ...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Sentiment\n",
       "405   i feel so honored today and i want to share th...       joy\n",
       "1189  i noticed that i was feeling very stressed and...     anger\n",
       "674                              i feel safe with berry       joy\n",
       "1358  im feeling quite positive in what i want to ac...       joy\n",
       "810   i want to say i feel numb but if i was numb i ...   sadness\n",
       "...                                                 ...       ...\n",
       "108   im feeling pretty disheartened by the whole thing   sadness\n",
       "1294           i start to feel lethargic about blogging   sadness\n",
       "1780  i had told gerry yesterday that if i feel isol...   sadness\n",
       "2     i left with my bouquet of red and yellow tulip...       joy\n",
       "1774  i type these words i feel like i shouldn t be ...  surprise\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the testing set to ~600 samples\n",
    "percentage = 0.3\n",
    "df_test_downsampled = df_test.sample(frac=percentage,random_state=0)\n",
    "df_test_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset has no NULL values.\n",
      "Testing Dataset has no NULL values.\n"
     ]
    }
   ],
   "source": [
    "# Check for Null Values\n",
    "if ( not df_train_downsampled.isnull().values.any() ):\n",
    "    print(\"Training Dataset has no NULL values.\")\n",
    "else:\n",
    "    print(\"Removing samples in Training Set with NULL values\")\n",
    "    df_train_downsampled.dropna(subset = [\"Text\"], inplace=True)\n",
    "    df_train_downsampled.dropna(subset = [\"Sentiment\"], inplace=True)\n",
    "    print(\"Null values Exist In X,Y Training Set:\", df_train_downsampled.isnull().values.any())\n",
    "\n",
    "if ( not df_test_downsampled.isnull().values.any() ):\n",
    "    print(\"Testing Dataset has no NULL values.\")\n",
    "else:\n",
    "    print(\"Removing samples in Testing Set with NULL values\")\n",
    "    df_test_downsampled.dropna(subset = [\"Text\"], inplace=True)\n",
    "    df_test_downsampled.dropna(subset = [\"Sentiment\"], inplace=True)\n",
    "    print(\"Null values Exist In X,Y Training Set:\", df_test_downsampled.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TrainX/Y and TestX/Y\n",
    "TrainXdf=df_train_downsampled['Text']\n",
    "TrainYdf=df_train_downsampled['Sentiment']\n",
    "\n",
    "TestXdf=df_test_downsampled['Text']\n",
    "TestYdf=df_test_downsampled['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visual of Training Samples' Outcomes - Even Distribution of Outcomes\n",
    "sent_count = TrainYdf.value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "w = 0.35  \n",
    "plt.bar(x=np.arange(len(sent_count)), height=sent_count, width = w)\n",
    "\n",
    "plt.xticks(np.arange(len(sent_count)), sent_count.index.tolist())\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Emotion Count')\n",
    "plt.title('Emotion Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8833     0\n",
       "223      0\n",
       "15839    1\n",
       "6321     1\n",
       "14912    1\n",
       "        ..\n",
       "12639    1\n",
       "15530    0\n",
       "10109    1\n",
       "760      4\n",
       "5655     3\n",
       "Name: Sentiment, Length: 2400, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sentiment values to numbers (i.e. joy is 0, sadness is 1, anger is 2, fear is 3, love is 4, surprise is 5)\n",
    "TrainYdf.replace({\"joy\": 0, \"sadness\": 1, \"anger\":2, \"fear\":3, \"love\":4, \"surprise\":5}, inplace=True)\n",
    "TestYdf.replace({\"joy\": 0, \"sadness\": 1, \"anger\":2, \"fear\":3, \"love\":4, \"surprise\":5}, inplace=True)\n",
    "TrainYdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 700)\n",
      "(600, 700)\n"
     ]
    }
   ],
   "source": [
    "# \"One-Hot Encoding\" Function that converts the Tweets to Numerical Arrays\n",
    "def oneHotEnc(Tweet):\n",
    "    token_item = []\n",
    "\n",
    "    for letter in Tweet:\n",
    "        if ord(letter) >= ord('a') and ord(letter) <= ord('z'):\n",
    "            token_item.append(ord(letter) - ord('a'))\n",
    "        elif letter == ' ':\n",
    "            token_item.append(-1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return token_item\n",
    "\n",
    "# Function that Pre-Processes the Tweets\n",
    "def PreProcess(tweets, method):\n",
    "    pre_procc_tweets = []\n",
    "\n",
    "    # Storing all punctuations using RE library like !;,\"% etc\n",
    "    re_puncs = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    # Storing all stop words like a, an, the, when, there, this etc\n",
    "    stop_word  = set(stopwords.words('english'))\n",
    "\n",
    "    # Lemmatizing object\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    # Using Porter Stemmer\n",
    "    p_stem = PorterStemmer()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        # Get words in tweet\n",
    "        words = word_tokenize(str(tweet))\n",
    "\n",
    "        # Converting all characters to lower case\n",
    "        words_lower = [w.lower() for w in words]\n",
    "\n",
    "        # Remove all punctuation\n",
    "        words_lower_no_punc = [re_puncs.sub('', w) for w in words_lower]\n",
    "\n",
    "        # Keep only alpha words\n",
    "        words_lower_alpha = [i for i in words_lower_no_punc if i.isalpha()]\n",
    "\n",
    "        # Removing all stop words\n",
    "        words_lower_alpha_nostop = [w for w in words_lower_alpha if w not in stop_word]\n",
    "\n",
    "        # Doing Lemmatizing of words\n",
    "        words_lower_alpha_nostop_lemma = [lem.lemmatize(w) for w in words_lower_alpha_nostop]\n",
    "\n",
    "        # Stemming process\n",
    "        words_lower_alpha_nostop_lemma_stem = [p_stem.stem(w) for w in words_lower_alpha_nostop_lemma]\n",
    "\n",
    "        # Convert back to string and (possibly) one-hot encode tweet\n",
    "        pre_procc_str = ' '.join(words_lower_alpha_nostop_lemma_stem)\n",
    "        if (method==1):\n",
    "            procc_tweet = oneHotEnc(pre_procc_str)\n",
    "        else:\n",
    "            procc_tweet=pre_procc_str\n",
    "        pre_procc_tweets.append(procc_tweet)\n",
    "        \n",
    "    return pre_procc_tweets\n",
    "\n",
    "# Encoding method (1 for NGRAM, 0 for TOKENIZATION)\n",
    "method = 0\n",
    "\n",
    "# Max Number of features for Tokenization\n",
    "max_features = 700\n",
    "\n",
    "# Pre-Process the Tweets and Get Finalized Training and Testing Sets\n",
    "TrainXdf = PreProcess(TrainXdf,method)\n",
    "TestXdf = PreProcess(TestXdf,method)\n",
    "\n",
    "if (method ==0):\n",
    "    cnt = CountVectorizer(analyzer=\"word\", ngram_range=(2,3), max_features=max_features)\n",
    "    TrainXdf = cnt.fit_transform(TrainXdf).toarray()\n",
    "    TestXdf=cnt.transform(TestXdf).toarray()\n",
    "\n",
    "TrainYdf=np.array(TrainYdf)\n",
    "TestYdf=np.array(TestYdf)\n",
    "\n",
    "print(TrainXdf.shape)\n",
    "print(TestXdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Memory Generation Function, NGRAM\n",
    "def itemMemGen_NGRAM(dim=10000, num_char=37):\n",
    "    dictMem = np.random.randint(2, size=(num_char, dim), dtype='int32')\n",
    "    return dictMem\n",
    "# Item Memory Generation Function, TOKENIZATION\n",
    "def itemMemGen_TOKEN(features, dim=10000):\n",
    "    dictMem = np.random.randint(2, size=(features, dim), dtype='int32')\n",
    "    return dictMem\n",
    "\n",
    "# Hyperparamaters\n",
    "HV_dim = 10000\n",
    "num_supported_chars = 27\n",
    "\n",
    "# Item Memory Generation\n",
    "if (method ==1):\n",
    "    itemMem = itemMemGen_NGRAM(dim=HV_dim, num_char=num_supported_chars)\n",
    "else:\n",
    "    itemMem = itemMemGen_TOKEN(features=len(TrainXdf[0]), dim=HV_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[604, 666, 728, ..., 686, 682, 506],\n",
       "       [470, 667, 732, ..., 648, 683, 526],\n",
       "       [253, 325, 309, ..., 272, 305, 244],\n",
       "       [257, 273, 257, ..., 265, 318, 261],\n",
       "       [134, 209, 222, ..., 215, 229, 140],\n",
       "       [ 50,  73,  62, ...,  64,  68,  59]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for Encoding a Tweet into a HV\n",
    "def encode(tweet, itemMem, method, HV_dim=10000, n_gram_len=3):\n",
    "    if (method==1):\n",
    "        tweet_HV = np.zeros(HV_dim, dtype='int32')\n",
    "\n",
    "        for ngram_start in range(0, len(tweet)-n_gram_len, 3):\n",
    "            roll_value=n_gram_len\n",
    "            for j in range(n_gram_len):\n",
    "                letterHV = itemMem[tweet[ngram_start + j]]\n",
    "\n",
    "                if (j==0):\n",
    "                    product = letterHV\n",
    "                elif (j==n_gram_len-1):\n",
    "                    product = product * np.roll(letterHV, roll_value)\n",
    "                    tweet_HV = np.add(tweet_HV, product)\n",
    "                else:\n",
    "                    product = product * np.roll(letterHV, roll_value)\n",
    "                    \n",
    "                roll_value = roll_value - 1\n",
    "\n",
    "        HV_avg = np.average(tweet_HV)\n",
    "        tweet_HV[tweet_HV > HV_avg] = 1\n",
    "        tweet_HV[tweet_HV < HV_avg] = -1\n",
    "        tweet_HV[tweet_HV == HV_avg] = 0\n",
    "        return tweet_HV\n",
    "    else:\n",
    "        return tweet.dot(itemMem)\n",
    "\n",
    "# HDC Training Function Creates Associative Memory\n",
    "def train(X, Y, itemMem, HV_dim, sent_count, n_gram_len, method):\n",
    "    assocMem = np.zeros((sent_count, HV_dim), dtype='int32')\n",
    "    encodedTweets=[]\n",
    "    tweet_idx = 0\n",
    "    \n",
    "    for tweet in X:\n",
    "        tweet_HV = encode(tweet.reshape(1, len(tweet)), itemMem, method, HV_dim, n_gram_len)\n",
    "        if (tweet_idx==0):\n",
    "            print(tweet_HV.shape)\n",
    "        assocMem[Y[tweet_idx]] = np.add(assocMem[Y[tweet_idx]], tweet_HV)\n",
    "        tweet_idx += 1\n",
    "\n",
    "    return assocMem\n",
    "\n",
    "n_gram_len = 3\n",
    "assocMem = train(TrainXdf, TrainYdf, itemMem, HV_dim, len(sent_count), n_gram_len, method)\n",
    "assocMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Shot Accuracy:  51.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# Function Compares Input HV to Class HVs and Returns the Predicted Class\n",
    "def get_prediction(assocMem, inputHV):\n",
    "    pred = assocMem[0]\n",
    "    maximum = np.NINF\n",
    "\n",
    "    for index in range(len(assocMem)):\n",
    "        similarity = cosine_similarity([inputHV, assocMem[index]])[0][1]  \n",
    "        if (similarity > maximum):\n",
    "            pred = index\n",
    "            maximum = similarity\n",
    "\n",
    "    return pred\n",
    "\n",
    "# Function Tests the Model and Return Accuracy of Model\n",
    "def test(HV_dim, n_gram_len, itemMem, assocMem, TestXdf, TestYdf, method):\n",
    "    correct_count = 0\n",
    "\n",
    "    for index in range(len(TestXdf)):\n",
    "        prediction = get_prediction(assocMem, encode(TestXdf[index], itemMem, method, HV_dim, n_gram_len))\n",
    "        if (TestYdf[index] == prediction):\n",
    "            correct_count += 1\n",
    "            \n",
    "    accuracy = (correct_count / len(TestYdf)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# One-Shot Training Results\n",
    "one_shot_accuracy=test(HV_dim, n_gram_len, itemMem, assocMem, TestXdf, TestYdf, method)\n",
    "print(\"One Shot Accuracy: \", one_shot_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Retraining Model:  20  Epochs --------\n",
      "start\n",
      "Epoch  1 :  48.0\n",
      "start\n",
      "Epoch  2 :  47.16666666666667\n",
      "start\n",
      "Epoch  3 :  51.33333333333333\n",
      "start\n",
      "Epoch  4 :  50.16666666666667\n",
      "start\n",
      "Epoch  5 :  50.0\n",
      "start\n",
      "Epoch  6 :  48.66666666666667\n",
      "start\n"
     ]
    }
   ],
   "source": [
    "def retrain(X, Y, itemMem, assocMem, HV_dim, n_gram_len, method, alpha):\n",
    "    tweet_index = 0\n",
    "    print(\"start\")\n",
    "    for tweet in X:\n",
    "        tweet_HV = encode(tweet, itemMem, method, HV_dim, n_gram_len)\n",
    "        prediction = get_prediction(assocMem, tweet_HV)\n",
    "        if prediction != Y[tweet_index]:\n",
    "            assocMem[Y[tweet_index]] = np.add(assocMem[Y[tweet_index]], alpha * tweet_HV)\n",
    "            assocMem[prediction] = np.subtract(assocMem[prediction], alpha * tweet_HV)\n",
    "        tweet_index += 1\n",
    "        if(tweet_index%10000 == 0):\n",
    "            print(\"still going\", tweet_index)\n",
    "    return assocMem\n",
    "\n",
    "# Re-Train Model\n",
    "num_epochs = 20\n",
    "print('-------- Retraining Model: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf ,itemMem, assocMem, HV_dim, n_gram_len, method, alpha = num_epochs - epoch)\n",
    "    acc = test(HV_dim, n_gram_len, itemMem, assocMem, TestXdf, TestYdf, method)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071eaeccc96c6410cecdb330bf8e8ae0267d24b86e05481c728d399cbe7cbc33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('aml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
